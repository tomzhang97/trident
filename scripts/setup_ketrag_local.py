#!/usr/bin/env python3
"""
KET-RAG Setup with Local LLM

This script sets up a KET-RAG project with local LLM support.
It follows the original KET-RAG workflow:
1. Initialize project (graphrag init)
2. Create settings.yaml for local LLM
3. Create .env file

Usage:
    python scripts/setup_ketrag_local.py \
        --root_path ragtest-musique \
        --llm_api_base http://localhost:8000/v1 \
        --llm_model meta-llama/Meta-Llama-3-8B-Instruct \
        --embedding_api_base http://localhost:8001/v1 \
        --embedding_model text-embedding-3-small

    # Or use local sentence-transformers for embeddings
    python scripts/setup_ketrag_local.py \
        --root_path ragtest-musique \
        --llm_api_base http://localhost:8000/v1 \
        --llm_model meta-llama/Meta-Llama-3-8B-Instruct \
        --use_local_embeddings \
        --local_embedding_model sentence-transformers/all-MiniLM-L6-v2
"""

import argparse
import os
import sys
from pathlib import Path


def create_settings_yaml(
    root_path: str,
    llm_api_base: str,
    llm_model: str,
    llm_api_key: str = "EMPTY",
    embedding_api_base: str = None,
    embedding_model: str = "text-embedding-3-small",
    embedding_api_key: str = None,
    use_local_embeddings: bool = False,
    local_embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
) -> str:
    """Generate settings.yaml content for local LLM."""

    # Embedding configuration
    if use_local_embeddings:
        embedding_config = f"""  default_embedding:
    type: sentence_transformers_embedding
    model: {local_embedding_model}
    # device: cuda:0  # Uncomment to specify device"""
    else:
        emb_api_base = embedding_api_base or llm_api_base
        emb_api_key = embedding_api_key or llm_api_key
        embedding_config = f"""  default_embedding:
    type: openai_embedding
    model_provider: openai
    auth_type: api_key
    api_key: {emb_api_key}
    model: {embedding_model}
    api_base: {emb_api_base}
    concurrent_requests: 25
    async_mode: threaded
    retry_strategy: native
    max_retries: 10"""

    settings = f"""### KET-RAG Settings for Local LLM ###
### Generated by setup_ketrag_local.py ###

### LLM settings ###
models:
  default_chat:
    type: openai_chat
    model_provider: openai
    auth_type: api_key
    api_key: {llm_api_key}
    model: {llm_model}
    api_base: {llm_api_base}
    model_supports_json: true
    concurrent_requests: 25
    async_mode: threaded
    retry_strategy: native
    max_retries: 10
    tokens_per_minute: null
    requests_per_minute: null
{embedding_config}

### Input settings ###
input:
  storage:
    type: file
    base_dir: "input"
  file_type: text

chunks:
  size: 1200
  overlap: 100
  group_by_columns: [id]

### Output/storage settings ###
output:
  type: file
  base_dir: "output"

cache:
  type: file
  base_dir: "cache"

reporting:
  type: file
  base_dir: "reports"

vector_store:
  default:
    type: lancedb
    db_uri: lancedb
    container_name: default

### Workflow settings ###
embed_text:
  model_id: default_embedding
  vector_store_id: default

extract_graph:
  model_id: default_chat
  prompt: "prompts/extract_graph.txt"
  entity_types: [organization, person, geo, event]
  max_gleanings: 1

summarize_descriptions:
  model_id: default_chat
  prompt: "prompts/summarize_descriptions.txt"
  max_length: 500

extract_graph_nlp:
  text_analyzer:
    extractor_type: regex_english
  async_mode: threaded

cluster_graph:
  max_cluster_size: 10

extract_claims:
  enabled: false
  model_id: default_chat
  prompt: "prompts/extract_claims.txt"
  description: "Any claims or facts that could be relevant to information discovery."
  max_gleanings: 1

community_reports:
  model_id: default_chat
  graph_prompt: "prompts/community_report_graph.txt"
  text_prompt: "prompts/community_report_text.txt"
  max_length: 2000
  max_input_length: 8000

embed_graph:
  enabled: false

umap:
  enabled: false

snapshots:
  graphml: true
  embeddings: false

### Query settings ###
local_search:
  chat_model_id: default_chat
  embedding_model_id: default_embedding
  prompt: "prompts/local_search_system_prompt.txt"

global_search:
  chat_model_id: default_chat
  map_prompt: "prompts/global_search_map_system_prompt.txt"
  reduce_prompt: "prompts/global_search_reduce_system_prompt.txt"
  knowledge_prompt: "prompts/global_search_knowledge_system_prompt.txt"

drift_search:
  chat_model_id: default_chat
  embedding_model_id: default_embedding
  prompt: "prompts/drift_search_system_prompt.txt"
  reduce_prompt: "prompts/drift_search_reduce_prompt.txt"

basic_search:
  chat_model_id: default_chat
  embedding_model_id: default_embedding
  prompt: "prompts/basic_search_system_prompt.txt"
"""
    return settings


def create_env_file(api_key: str = "EMPTY") -> str:
    """Generate .env file content."""
    return f"GRAPHRAG_API_KEY={api_key}\n"


def main():
    parser = argparse.ArgumentParser(description="Setup KET-RAG project with local LLM")
    parser.add_argument("--root_path", required=True, help="Project root directory")

    # LLM settings
    parser.add_argument("--llm_api_base", required=True,
                        help="LLM API base URL (e.g., http://localhost:8000/v1)")
    parser.add_argument("--llm_model", required=True,
                        help="LLM model name (e.g., meta-llama/Meta-Llama-3-8B-Instruct)")
    parser.add_argument("--llm_api_key", default="EMPTY",
                        help="LLM API key (default: EMPTY for local)")

    # Embedding settings
    parser.add_argument("--embedding_api_base", default=None,
                        help="Embedding API base URL (default: same as LLM)")
    parser.add_argument("--embedding_model", default="text-embedding-3-small",
                        help="Embedding model name")
    parser.add_argument("--embedding_api_key", default=None,
                        help="Embedding API key")
    parser.add_argument("--use_local_embeddings", action="store_true",
                        help="Use local sentence-transformers for embeddings")
    parser.add_argument("--local_embedding_model",
                        default="sentence-transformers/all-MiniLM-L6-v2",
                        help="Local embedding model (sentence-transformers)")

    # Control flags
    parser.add_argument("--skip_init", action="store_true",
                        help="Skip graphrag init (use if already initialized)")
    parser.add_argument("--skip_prompt_tune", action="store_true",
                        help="Skip prompt tuning step")

    args = parser.parse_args()

    root_path = Path(args.root_path)

    # Step 1: Create directory structure
    print(f"Setting up KET-RAG project at {root_path}")
    root_path.mkdir(parents=True, exist_ok=True)
    (root_path / "input").mkdir(exist_ok=True)
    (root_path / "output").mkdir(exist_ok=True)
    (root_path / "cache").mkdir(exist_ok=True)
    (root_path / "reports").mkdir(exist_ok=True)
    (root_path / "prompts").mkdir(exist_ok=True)
    (root_path / "qa-pairs").mkdir(exist_ok=True)

    # Step 2: Run graphrag init if not skipping
    if not args.skip_init:
        print("\nStep 1: Initializing GraphRAG project...")
        import subprocess
        result = subprocess.run(
            ["python", "-m", "graphrag", "init", "--root", str(root_path)],
            capture_output=True, text=True
        )
        if result.returncode != 0:
            print(f"Warning: graphrag init returned non-zero: {result.stderr}")
        else:
            print("  GraphRAG initialized successfully")

    # Step 3: Create settings.yaml
    print("\nStep 2: Creating settings.yaml for local LLM...")
    settings_content = create_settings_yaml(
        root_path=str(root_path),
        llm_api_base=args.llm_api_base,
        llm_model=args.llm_model,
        llm_api_key=args.llm_api_key,
        embedding_api_base=args.embedding_api_base,
        embedding_model=args.embedding_model,
        embedding_api_key=args.embedding_api_key,
        use_local_embeddings=args.use_local_embeddings,
        local_embedding_model=args.local_embedding_model,
    )

    settings_path = root_path / "settings.yaml"
    with open(settings_path, "w") as f:
        f.write(settings_content)
    print(f"  Created {settings_path}")

    # Step 4: Create .env file
    env_path = root_path / ".env"
    with open(env_path, "w") as f:
        f.write(create_env_file(args.llm_api_key))
    print(f"  Created {env_path}")

    # Print next steps
    print("\n" + "="*60)
    print("SETUP COMPLETE!")
    print("="*60)
    print(f"""
Next steps:

1. Place your input documents in:
   {root_path}/input/

2. Place your QA pairs in:
   {root_path}/qa-pairs/qa-pairs.json
   (Format: [{{"id": "...", "question": "...", "answer": "..."}}])

3. Start your vLLM server:
   python -m vllm.entrypoints.openai.api_server \\
       --model {args.llm_model} \\
       --port 8000

4. (Optional) Tune prompts:
   python -m graphrag prompt-tune \\
       --root {root_path} \\
       --config {root_path}/settings.yaml \\
       --discover-entity-types

5. Build the index:
   python -m graphrag index --root {root_path}

6. Generate context:
   python KET-RAG/indexing_sket/create_context.py {root_path} keyword 0.5

   Or use local embeddings:
   python scripts/create_ketrag_context_local.py \\
       --root_path {root_path} \\
       --strategy keyword \\
       --budget 0.5

7. Generate answers:
   python scripts/llm_answer_local.py {root_path}

   Or with OpenAI API:
   export GRAPHRAG_API_KEY=your_key
   python KET-RAG/indexing_sket/llm_answer.py {root_path}
""")


if __name__ == "__main__":
    main()
